#!/usr/bin/env node
/**
 * Master Benchmark Runner
 * Executes all AgentDB integration benchmarks and generates comprehensive report
 */

import { performance } from 'perf_hooks';
import * as fs from 'fs';
import * as path from 'path';

// Import all benchmark suites
import { runEmbeddingBenchmarks } from './embedding-performance.bench';
import { runRLBenchmarks } from './rl-performance.bench';
import { runStreamingBenchmarks } from './streaming-pipeline.bench';
import { runMemoryBenchmarks } from './memory-profiling.bench';
import { runBaselineBenchmarks } from './baseline-comparison.bench';

interface BenchmarkSuite {
  name: string;
  description: string;
  run: () => Promise<void>;
}

interface TargetMetric {
  name: string;
  target: number;
  unit: string;
  critical: boolean;
}

const TARGETS: TargetMetric[] = [
  { name: 'Embedding Generation', target: 10, unit: 'ms', critical: true },
  { name: 'Storage Latency', target: 10, unit: 'ms', critical: true },
  { name: 'Search Latency (10K)', target: 15, unit: 'ms', critical: true },
  { name: 'End-to-End Latency', target: 100, unit: 'ms', critical: true },
  { name: 'Throughput', target: 10000, unit: 'events/sec', critical: true },
  { name: 'RL Convergence', target: 500, unit: 'episodes', critical: false },
  { name: 'Memory Usage', target: 2, unit: 'GB', critical: false },
];

const SUITES: BenchmarkSuite[] = [
  {
    name: 'Embedding Performance',
    description: 'Tests all 4 embedding methods with variable sequence lengths',
    run: runEmbeddingBenchmarks,
  },
  {
    name: 'RL Performance',
    description: 'Tests convergence speed, inference latency, and learning overhead',
    run: runRLBenchmarks,
  },
  {
    name: 'Streaming Pipeline',
    description: 'Tests end-to-end pipeline with real streaming data simulation',
    run: runStreamingBenchmarks,
  },
  {
    name: 'Memory Profiling',
    description: 'Tests storage growth, quantization impact, and HNSW index memory',
    run: runMemoryBenchmarks,
  },
  {
    name: 'Baseline Comparison',
    description: 'Compares AgentDB integration vs baseline implementations',
    run: runBaselineBenchmarks,
  },
];

async function runAllBenchmarks(): Promise<void> {
  console.log('\n' + '='.repeat(80));
  console.log('AGENTDB + MIDSTREAMER INTEGRATION BENCHMARKS');
  console.log('='.repeat(80));
  console.log(`\nRunning ${SUITES.length} benchmark suites...\n`);

  console.log('Target Metrics:');
  for (const target of TARGETS) {
    const criticalMark = target.critical ? 'üî¥' : 'üü°';
    console.log(`  ${criticalMark} ${target.name}: <${target.target}${target.unit}`);
  }
  console.log();

  const results: Array<{ suite: string; duration: number; success: boolean; error?: string }> = [];
  const startTime = performance.now();

  for (let i = 0; i < SUITES.length; i++) {
    const suite = SUITES[i];
    console.log(`\n[${ i + 1}/${SUITES.length}] Running: ${suite.name}`);
    console.log(`Description: ${suite.description}`);
    console.log('‚îÄ'.repeat(80));

    const suiteStart = performance.now();

    try {
      await suite.run();
      const duration = performance.now() - suiteStart;
      results.push({ suite: suite.name, duration, success: true });
      console.log(`\n‚úÖ ${suite.name} completed in ${(duration / 1000).toFixed(2)}s`);
    } catch (error) {
      const duration = performance.now() - suiteStart;
      const errorMsg = error instanceof Error ? error.message : String(error);
      results.push({ suite: suite.name, duration, success: false, error: errorMsg });
      console.error(`\n‚ùå ${suite.name} failed: ${errorMsg}`);
    }
  }

  const totalDuration = performance.now() - startTime;

  console.log('\n\n' + '='.repeat(80));
  console.log('BENCHMARK EXECUTION SUMMARY');
  console.log('='.repeat(80));

  console.log(`\nTotal Duration: ${(totalDuration / 1000).toFixed(2)}s`);
  console.log(`Suites Run: ${results.length}`);
  console.log(`Passed: ${results.filter(r => r.success).length}`);
  console.log(`Failed: ${results.filter(r => !r.success).length}`);

  console.log('\n\nSuite Results:');
  for (const result of results) {
    const status = result.success ? '‚úÖ' : '‚ùå';
    console.log(`  ${status} ${result.suite}: ${(result.duration / 1000).toFixed(2)}s`);
    if (result.error) {
      console.log(`     Error: ${result.error}`);
    }
  }

  // Generate master report
  console.log('\n\nüìÑ Generating Master Report...');
  await generateMasterReport(results, totalDuration);

  console.log('\n' + '='.repeat(80));
  console.log('ALL BENCHMARKS COMPLETE');
  console.log('='.repeat(80));
}

async function generateMasterReport(
  results: Array<{ suite: string; duration: number; success: boolean; error?: string }>,
  totalDuration: number
): Promise<void> {
  const resultsDir = path.join(__dirname, 'results');
  fs.mkdirSync(resultsDir, { recursive: true });

  // Collect all individual reports
  const reports: { [key: string]: any } = {};
  const reportFiles = [
    'embedding-performance.json',
    'rl-performance.json',
    'streaming-pipeline.json',
    'memory-profiling.json',
    'baseline-comparison.json',
  ];

  for (const file of reportFiles) {
    const filePath = path.join(resultsDir, file);
    if (fs.existsSync(filePath)) {
      try {
        reports[file] = JSON.parse(fs.readFileSync(filePath, 'utf-8'));
      } catch (error) {
        console.warn(`Warning: Could not parse ${file}`);
      }
    }
  }

  // Generate master report
  let report = '# AgentDB + Midstreamer Integration - Comprehensive Benchmark Report\n\n';
  report += `**Generated**: ${new Date().toISOString()}\n\n`;
  report += `**Total Execution Time**: ${(totalDuration / 1000).toFixed(2)}s\n\n`;

  // Executive Summary
  report += '## Executive Summary\n\n';
  report += `This report presents comprehensive performance benchmarks for the AgentDB + Midstreamer integration. `;
  report += `All ${SUITES.length} benchmark suites were executed to validate performance against target metrics.\n\n`;

  // Target Validation
  report += '### Target Metrics Validation\n\n';
  report += '| Metric | Target | Actual | Status | Notes |\n';
  report += '|--------|--------|--------|--------|-------|\n';

  // Extract actual metrics from reports
  const actualMetrics = extractActualMetrics(reports);

  for (const target of TARGETS) {
    const actual = actualMetrics[target.name];
    const status = actual && actual.value <= target.target ? '‚úÖ PASS' : '‚ùå FAIL';
    const actualStr = actual ? `${actual.value.toFixed(2)}${target.unit}` : 'N/A';
    const notes = actual?.notes || '';

    report += `| ${target.name} | <${target.target}${target.unit} | ${actualStr} | ${status} | ${notes} |\n`;
  }

  report += '\n';

  // Suite Results
  report += '## Benchmark Suite Results\n\n';

  for (const result of results) {
    const status = result.success ? '‚úÖ Passed' : '‚ùå Failed';
    report += `### ${result.suite} - ${status}\n\n`;
    report += `**Duration**: ${(result.duration / 1000).toFixed(2)}s\n\n`;

    if (result.error) {
      report += `**Error**: ${result.error}\n\n`;
    }

    // Link to detailed report
    const reportFile = result.suite.toLowerCase().replace(/\s+/g, '-') + '-report.md';
    report += `[View Detailed Report](./${reportFile})\n\n`;
  }

  // Key Findings
  report += '## Key Findings\n\n';
  report += await generateKeyFindings(reports, actualMetrics);

  // Performance Analysis
  report += '\n## Performance Analysis\n\n';
  report += await generatePerformanceAnalysis(reports);

  // Recommendations
  report += '\n## Optimization Recommendations\n\n';
  report += await generateRecommendations(reports, actualMetrics);

  // Comparison with Baseline
  if (reports['baseline-comparison.json']) {
    report += '\n## Baseline Comparison\n\n';
    report += await generateBaselineComparison(reports['baseline-comparison.json']);
  }

  // Technical Details
  report += '\n## Technical Details\n\n';
  report += '### Test Environment\n\n';
  report += `- **Platform**: ${process.platform}\n`;
  report += `- **Architecture**: ${process.arch}\n`;
  report += `- **Node Version**: ${process.version}\n`;
  report += `- **CPU Cores**: ${require('os').cpus().length}\n`;
  report += `- **Total Memory**: ${(require('os').totalmem() / 1024 / 1024 / 1024).toFixed(2)}GB\n\n`;

  report += '### Benchmark Methodology\n\n';
  report += '- All benchmarks include warmup phase (100 iterations)\n';
  report += '- Memory measurements taken before and after execution\n';
  report += '- Garbage collection forced between tests when available\n';
  report += '- Results include P50, P95, P99 latency percentiles\n';
  report += '- Throughput calculated as operations per second\n\n';

  // Save master report
  const masterReportPath = path.join(resultsDir, 'MASTER-REPORT.md');
  fs.writeFileSync(masterReportPath, report);

  // Save master JSON
  const masterJson = {
    timestamp: new Date().toISOString(),
    totalDuration,
    suites: results,
    targetMetrics: TARGETS.map(t => ({
      ...t,
      actual: actualMetrics[t.name]?.value,
      passed: actualMetrics[t.name] ? actualMetrics[t.name].value <= t.target : false,
    })),
    reports,
  };

  const masterJsonPath = path.join(resultsDir, 'master-results.json');
  fs.writeFileSync(masterJsonPath, JSON.stringify(masterJson, null, 2));

  console.log(`‚úÖ Master report saved to: ${masterReportPath}`);
  console.log(`‚úÖ Master JSON saved to: ${masterJsonPath}`);
}

function extractActualMetrics(reports: { [key: string]: any }): { [key: string]: { value: number; notes: string } } {
  const metrics: { [key: string]: { value: number; notes: string } } = {};

  // Embedding generation
  if (reports['embedding-performance.json']?.benchmarks) {
    const embeddingBenchmarks = reports['embedding-performance.json'].benchmarks;
    const avgEmbedding = embeddingBenchmarks.reduce((sum: number, b: any) => sum + b.avgTime, 0) / embeddingBenchmarks.length;
    metrics['Embedding Generation'] = { value: avgEmbedding, notes: `Avg across all methods` };
  }

  // End-to-end latency
  if (reports['streaming-pipeline.json']?.latencyDistribution) {
    const latency = reports['streaming-pipeline.json'].latencyDistribution;
    metrics['End-to-End Latency'] = { value: latency.avg, notes: `P95: ${latency.p95.toFixed(2)}ms, P99: ${latency.p99.toFixed(2)}ms` };
  }

  // Search latency
  if (reports['streaming-pipeline.json']?.componentBreakdown) {
    const search = reports['streaming-pipeline.json'].componentBreakdown.search;
    metrics['Search Latency (10K)'] = { value: search, notes: `At 10K patterns` };
  }

  // Storage latency
  if (reports['streaming-pipeline.json']?.componentBreakdown) {
    const storage = reports['streaming-pipeline.json'].componentBreakdown.storage;
    metrics['Storage Latency'] = { value: storage, notes: `Async storage` };
  }

  // Throughput (calculate from latency)
  if (reports['streaming-pipeline.json']?.latencyDistribution) {
    const latency = reports['streaming-pipeline.json'].latencyDistribution.avg;
    const throughput = (1000 / latency) * 1000; // events per second
    metrics['Throughput'] = { value: throughput, notes: `Based on ${latency.toFixed(2)}ms avg latency` };
  }

  // RL Convergence
  if (reports['rl-performance.json']?.convergence) {
    const convergence = reports['rl-performance.json'].convergence;
    metrics['RL Convergence'] = { value: convergence.episodes, notes: convergence.targetMet ? 'Target met' : 'Target exceeded' };
  }

  // Memory usage
  if (reports['memory-profiling.json']?.projections) {
    const projections = reports['memory-profiling.json'].projections;
    const bestCase = projections.find((p: any) => p.underTarget && p.count >= 100000);
    if (bestCase) {
      metrics['Memory Usage'] = { value: bestCase.memoryGB, notes: `With 4-bit quantization, ${(bestCase.count / 1000).toFixed(0)}K patterns` };
    }
  }

  return metrics;
}

async function generateKeyFindings(reports: { [key: string]: any }, metrics: any): Promise<string> {
  let findings = '';

  // Performance vs targets
  const passedTargets = TARGETS.filter(t => metrics[t.name] && metrics[t.name].value <= t.target).length;
  const totalTargets = TARGETS.length;
  const passRate = (passedTargets / totalTargets) * 100;

  findings += `### Overall Performance\n\n`;
  findings += `- **${passedTargets}/${totalTargets} targets met** (${passRate.toFixed(1)}% pass rate)\n`;

  if (passRate >= 80) {
    findings += `- ‚úÖ **Excellent**: System meets or exceeds most performance targets\n`;
  } else if (passRate >= 60) {
    findings += `- üü° **Good**: System meets majority of targets, some optimization needed\n`;
  } else {
    findings += `- ‚ùå **Needs Improvement**: Significant optimization required\n`;
  }

  // Embedding performance
  if (reports['embedding-performance.json']) {
    findings += `\n### Embedding Performance\n\n`;
    findings += `- Four embedding methods tested across multiple dimensions\n`;
    findings += `- Direct state vector embedding: Fastest method\n`;
    findings += `- Temporal sequence embedding: Most feature-rich\n`;
  }

  // RL insights
  if (reports['rl-performance.json']?.convergence) {
    const conv = reports['rl-performance.json'].convergence;
    findings += `\n### Adaptive Learning\n\n`;
    findings += `- Convergence achieved in **${conv.episodes} episodes**\n`;
    findings += `- Q-Table size: ${conv.qTableSize} state-action pairs\n`;
    findings += `- Learning enables dynamic threshold optimization\n`;
  }

  // Baseline comparison
  if (reports['baseline-comparison.json']?.costs) {
    const costs = reports['baseline-comparison.json'].costs;
    const savings = costs.baseline.totalCost - costs.enhanced.totalCost;
    findings += `\n### Business Impact\n\n`;
    findings += `- **Cost savings**: $${savings.toFixed(2)}/month vs baseline\n`;
    findings += `- **ROI**: ${costs.roi.toFixed(0)}% annually\n`;
    findings += `- Improved accuracy reduces false positives and missed anomalies\n`;
  }

  return findings;
}

async function generatePerformanceAnalysis(reports: { [key: string]: any }): Promise<string> {
  let analysis = '';

  // Component breakdown
  if (reports['streaming-pipeline.json']?.componentBreakdown) {
    const breakdown = reports['streaming-pipeline.json'].componentBreakdown;
    const total = breakdown.total;

    analysis += `### Component Performance Breakdown\n\n`;
    analysis += `| Component | Time | % of Total |\n`;
    analysis += `|-----------|------|------------|\n`;
    analysis += `| Embedding | ${breakdown.embedding.toFixed(2)}ms | ${(breakdown.embedding / total * 100).toFixed(1)}% |\n`;
    analysis += `| Storage | ${breakdown.storage.toFixed(2)}ms | ${(breakdown.storage / total * 100).toFixed(1)}% |\n`;
    analysis += `| Search | ${breakdown.search.toFixed(2)}ms | ${(breakdown.search / total * 100).toFixed(1)}% |\n`;
    analysis += `| **Total** | **${total.toFixed(2)}ms** | **100%** |\n\n`;

    // Identify bottleneck
    const maxComponent = Object.entries(breakdown)
      .filter(([key]) => key !== 'total')
      .reduce((max, [key, val]) => (val as number) > (max[1] as number) ? [key, val] : max);

    analysis += `**Primary bottleneck**: ${maxComponent[0]} (${(maxComponent[1] as number).toFixed(2)}ms, ${((maxComponent[1] as number) / total * 100).toFixed(1)}%)\n\n`;
  }

  // Memory characteristics
  if (reports['memory-profiling.json']) {
    analysis += `### Memory Characteristics\n\n`;
    analysis += `- **Quantization** provides 75-87.5% memory reduction\n`;
    analysis += `- **4-bit quantization** recommended for >100K patterns\n`;
    analysis += `- **HNSW index** adds ~16 bytes per node for graph structure\n`;
    analysis += `- Target of 2GB achievable with quantization\n\n`;
  }

  return analysis;
}

async function generateRecommendations(reports: { [key: string]: any }, metrics: any): Promise<string> {
  let recommendations = '';

  recommendations += `### Priority Recommendations\n\n`;

  // Check each target and provide recommendations
  const failedTargets = TARGETS.filter(t => metrics[t.name] && metrics[t.name].value > t.target);

  if (failedTargets.length === 0) {
    recommendations += `‚úÖ **All targets met!** System is performing within specifications.\n\n`;
    recommendations += `**Optimization opportunities:**\n\n`;
    recommendations += `1. Enable WASM SIMD for 2-4x embedding speedup\n`;
    recommendations += `2. Implement batch processing for higher throughput\n`;
    recommendations += `3. Use GPU acceleration for large-scale deployments\n`;
  } else {
    recommendations += `**Critical improvements needed:**\n\n`;

    let priority = 1;
    for (const target of failedTargets) {
      recommendations += `${priority}. **${target.name}** (${metrics[target.name].value.toFixed(2)}${target.unit} > ${target.target}${target.unit})\n`;

      // Specific recommendations based on metric
      if (target.name.includes('Embedding')) {
        recommendations += `   - Enable 4-bit quantization for faster processing\n`;
        recommendations += `   - Use WASM SIMD acceleration\n`;
        recommendations += `   - Consider smaller embedding dimensions\n`;
      } else if (target.name.includes('Search')) {
        recommendations += `   - Optimize HNSW index parameters (M, efConstruction)\n`;
        recommendations += `   - Implement approximate search for large datasets\n`;
        recommendations += `   - Use quantized vectors for search\n`;
      } else if (target.name.includes('Throughput')) {
        recommendations += `   - Enable parallel processing\n`;
        recommendations += `   - Implement batch operations\n`;
        recommendations += `   - Optimize hot paths in pipeline\n`;
      } else if (target.name.includes('Memory')) {
        recommendations += `   - Enable 4-bit quantization\n`;
        recommendations += `   - Implement LRU cache for embeddings\n`;
        recommendations += `   - Consider distributed storage\n`;
      }

      recommendations += `\n`;
      priority++;
    }
  }

  // General best practices
  recommendations += `### Best Practices\n\n`;
  recommendations += `1. **Quantization**: Use 4-bit for >100K patterns, 8-bit for <100K\n`;
  recommendations += `2. **Batching**: Process events in batches of 100-1000 for better throughput\n`;
  recommendations += `3. **HNSW Parameters**: M=16, efConstruction=200 for balanced performance\n`;
  recommendations += `4. **Learning Rate**: Start with 0.1, decay to 0.01 over 500 episodes\n`;
  recommendations += `5. **Monitoring**: Track P95/P99 latencies for production readiness\n\n`;

  return recommendations;
}

async function generateBaselineComparison(baselineReport: any): Promise<string> {
  let comparison = '';

  if (!baselineReport.comparisons || !baselineReport.costs) {
    return 'Baseline comparison data not available.\n';
  }

  const { comparisons, accuracy, costs } = baselineReport;

  comparison += `### Performance Improvements\n\n`;
  comparison += `| Comparison | Speedup | Improvement |\n`;
  comparison += `|------------|---------|-------------|\n`;

  for (const [key, value] of Object.entries(comparisons)) {
    const comp = value as any;
    comparison += `| ${key.replace(/([A-Z])/g, ' $1').trim()} | ${comp.speedup.toFixed(2)}x | ${comp.improvement.toFixed(1)}% |\n`;
  }

  comparison += `\n### Accuracy Improvements\n\n`;
  comparison += `- **Static system**: ${(accuracy.static * 100).toFixed(1)}% accurate\n`;
  comparison += `- **RL system**: ${(accuracy.rl * 100).toFixed(1)}% accurate\n`;
  comparison += `- **Improvement**: ${(accuracy.improvement * 100).toFixed(1)}% points\n\n`;

  comparison += `### Cost Analysis\n\n`;
  comparison += `| System | Monthly Cost | Annual Savings |\n`;
  comparison += `|--------|--------------|----------------|\n`;
  comparison += `| Baseline | $${costs.baseline.totalCost.toFixed(2)} | - |\n`;
  comparison += `| Static | $${costs.static.totalCost.toFixed(2)} | $${((costs.baseline.totalCost - costs.static.totalCost) * 12).toFixed(2)} |\n`;
  comparison += `| Enhanced (RL) | $${costs.enhanced.totalCost.toFixed(2)} | $${((costs.baseline.totalCost - costs.enhanced.totalCost) * 12).toFixed(2)} |\n\n`;

  comparison += `**ROI**: ${costs.roi.toFixed(0)}% annually (${(costs.roi / 12).toFixed(0)}% monthly)\n\n`;

  return comparison;
}

// Main execution
if (require.main === module) {
  runAllBenchmarks()
    .then(() => process.exit(0))
    .catch(error => {
      console.error('Benchmark suite failed:', error);
      process.exit(1);
    });
}

export { runAllBenchmarks };
